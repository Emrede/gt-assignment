# GT-Assignment #

This project is an AWS Lambda function that processes CSV files uploaded to an S3 bucket, parses and processes the data, and sends the result to an SQS queue. The Lambda function is triggered by an S3 event when a new CSV file is uploaded to the specified bucket.

![alt text](gt-assignment-infra.jpg)

## Requirements ##
- AWS CLI
- Python 3
- Terraform

## AWS Resources ##
- Amazon S3 Bucket (`"aws_s3_bucket" "data_bucket"`): The CSV files are uploaded to this bucket, triggering the Lambda function.
- AWS Lambda Function (`"aws_lambda_function" "data_processor"`): Processes the CSV files and sends the result to an SQS queue.
- Amazon SQS Queue (`"aws_sqs_queue" "data_queue"`): Stores the processed messages generated by the Lambda function.
- AWS IAM Role (`"aws_iam_role" "lambda_role"`): Provides the necessary permissions for the Lambda function to interact with other AWS resources.


## Terraform Configuration ##
This Terraform configuration creates an S3 bucket, a Lambda function, an SQS queue, a DynamoDB instance to write the data and the necessary IAM roles and policies for the Lambda function to access the S3 bucket and SQS queue.

## Installation ##
1. Install the AWS CLI, Python 3, and Terraform if they're not already installed
2. Configure AWS with `aws configure` or export following values on your command line:
```
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
```

3. Before running Terraform, make sure a Lambda package with your Lambda function was created:
- Make sure there is a ZIP file containing your `lambda_function.py` file.
- Make sure the `lambda-function.zip` in the Terraform configuration to match the name of your ZIP file.
- Then, run `terraform init`, `terraform plan` and `terraform apply` to create the resources.

- With this setup, the data processing pipeline is ready, and you can test it by uploading CSV files to the S3 bucket. Example data converted to .csv files for test purposes

- After running `terraform apply`, edit `copy_csvs_to_bucket.sh` file and enter your bucket name as following `BUCKET_NAME="your-bucket-name"`
- Then run `copy_csvs_to_bucket.sh` to copy needed files to your bucket.

## Usage ##
- Lambda function can be run on AWS Console by creating a test event from:
`Lambda => Functions => data_processing`

- Or by the use of AWS CLI:
`aws lambda invoke --function-name data_processing --payload file://s3_event.json output.txt`

The Lambda function processes CSV files with the following format:

```
customer_id;first_name;last_name;customer_reference;status
order_id;customer_reference;order_status;order_reference;order_timestamp
item_id;order_reference;item_name;quantity;total_price
```

## Cleanup ##
- To be able to run `terraform destroy`, you should empty your bucket first from AWS Console. Otherwise, it won't be able to destroy created bucket.
